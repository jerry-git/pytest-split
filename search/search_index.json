{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pytest-split","text":"<p>Documentation: https://jerry-git.github.io/pytest-split</p> <p>Source Code: https://github.com/jerry-git/pytest-split</p> <p>PyPI: https://pypi.org/project/pytest-split/</p> <p>Pytest plugin which splits the test suite to equally sized \"sub suites\" based on test execution time.</p>"},{"location":"#motivation","title":"Motivation","text":"<ul> <li>Splitting the test suite is a prerequisite for parallelization (who does not want faster CI builds?). It's valuable to have sub suites which execution time is around the same.</li> <li><code>pytest-test-groups</code> is great but it does not take into account the execution time of sub suites which can lead to notably unbalanced execution times between the sub suites.</li> <li><code>pytest-xdist</code> is great but it's not suitable for all use cases. For example, some test suites may be fragile considering the order in which the tests are executed. This is of course a fundamental problem in the suite itself but sometimes it's not worth the effort to refactor, especially if the suite is huge (and smells a bit like legacy). Additionally, <code>pytest-split</code> may be a better fit in some use cases considering distributed execution.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pytest-split\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>First we have to store test durations from a complete test suite run. This produces .test_durations file which should be stored in the repo in order to have it available during future test runs. The file path is configurable via <code>--durations-path</code> CLI option.</p> <pre><code>pytest --store-durations\n</code></pre> <p>Then we can have as many splits as we want:</p> <pre><code>pytest --splits 3 --group 1\npytest --splits 3 --group 2\npytest --splits 3 --group 3\n</code></pre> <p>Time goes by, new tests are added and old ones are removed/renamed during development. No worries! <code>pytest-split</code> assumes average test execution time (calculated based on the stored information) for every test which does not have duration information stored. Thus, there's no need to store durations after changing the test suite. However, when there are major changes in the suite compared to what's stored in .test_durations, it's recommended to update the duration information with <code>--store-durations</code> to ensure that the splitting is in balance.</p> <p>The splitting algorithm can be controlled with the <code>--splitting-algorithm</code> CLI option and defaults to <code>duration_based_chunks</code>. For more information about the different algorithms and their tradeoffs, please see the section below.</p>"},{"location":"#cli-commands","title":"CLI commands","text":""},{"location":"#slowest-tests","title":"slowest-tests","text":"<p>Lists the slowest tests based on the information stored in the test durations file. See <code>slowest-tests --help</code> for more  information.</p>"},{"location":"#interactions-with-other-pytest-plugins","title":"Interactions with other pytest plugins","text":"<ul> <li> <p><code>pytest-random-order</code> and <code>pytest-randomly</code>:    \u26a0\ufe0f <code>pytest-split</code> running with the <code>duration_based_chunks</code> algorithm is incompatible with test-order-randomization plugins.   Test selection in the groups happens after randomization, potentially causing some tests to be selected in several groups and others not at all.   Instead, a global random seed needs to be computed before running the tests (for example using <code>$RANDOM</code> from the shell) and that single seed then needs to be used for all groups by setting the <code>--random-order-seed</code> option.</p> </li> <li> <p><code>nbval</code>: <code>pytest-split</code> could, in principle, break up a single IPython Notebook into different test groups. This most likely causes broken up pieces to fail (for the very least, package <code>import</code>s are usually done at Cell 1, and so, any broken up piece that doesn't contain Cell 1 will certainly fail). To avoid this, after splitting step is done, test groups are reorganized based on a simple algorithm illustrated in the following cartoon:</p> </li> </ul> <p></p> <p>where the letters (A to E) refer to individual IPython Notebooks, and the numbers refer to the corresponding cell number.</p>"},{"location":"#splitting-algorithms","title":"Splitting algorithms","text":"<p>The plugin supports multiple algorithms to split tests into groups. Each algorithm makes different tradeoffs, but generally <code>least_duration</code> should give more balanced groups.</p> Algorithm Maintains Absolute Order Maintains Relative Order Split Quality Works with random ordering duration_based_chunks \u2705 \u2705 Good \u274c least_duration \u274c \u2705 Better \u2705 <p>Explanation of the terms in the table: * Absolute Order: whether each group contains all tests between first and last element in the same order as the original list of tests * Relative Order: whether each test in each group has the same relative order to its neighbours in the group as in the original list of tests * Works with random ordering: whether the algorithm works with test-shuffling tools such as <code>pytest-randomly</code></p> <p>The <code>duration_based_chunks</code> algorithm aims to find optimal boundaries for the list of tests and every test group contains all tests between the start and end boundary. The <code>least_duration</code> algorithm walks the list of tests and assigns each test to the group with the smallest current duration.</p> <p>Demo with GitHub Actions</p>"},{"location":"#development","title":"Development","text":"<ul> <li>Clone this repository</li> <li>Requirements:</li> <li>Poetry</li> <li>Python 3.8+</li> <li>Create a virtual environment and install the dependencies</li> </ul> <pre><code>poetry install\n</code></pre> <ul> <li>Activate the virtual environment</li> </ul> <pre><code>poetry shell\n</code></pre>"},{"location":"#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation is automatically generated from the content of the docs directory and from the docstrings  of the public signatures of the source code. The documentation is updated and published as a Github Pages page automatically as part each release.</p>"},{"location":"#releasing","title":"Releasing","text":"<p>Trigger the Draft release workflow (press Run workflow). This will update the changelog &amp; version and create a GitHub release which is in Draft state.</p> <p>Find the draft release from the GitHub releases and publish it. When  a release is published, it'll trigger release workflow which creates PyPI  release and deploys updated documentation.</p>"},{"location":"#pre-commit","title":"Pre-commit","text":"<p>Pre-commit hooks run all the auto-formatting (<code>ruff format</code>), linters (e.g. <code>ruff</code> and <code>mypy</code>), and other quality  checks to make sure the changeset is in good shape before a commit/push happens.</p> <p>You can install the hooks with (runs for each commit):</p> <pre><code>pre-commit install\n</code></pre> <p>Or if you want them to run only for each push:</p> <pre><code>pre-commit install -t pre-push\n</code></pre> <p>Or if you want e.g. want to run all checks manually for all files:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>This project was generated using the wolt-python-package-cookiecutter template.</p>"},{"location":"api_docs/","title":"API documentation","text":""},{"location":"api_docs/#pytest_split.algorithms","title":"<code>algorithms</code>","text":""},{"location":"api_docs/#pytest_split.algorithms.AlgorithmBase","title":"<code>AlgorithmBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for the algorithm implementations.</p> Source code in <code>src/pytest_split/algorithms.py</code> <pre><code>class AlgorithmBase(ABC):\n    \"\"\"Abstract base class for the algorithm implementations.\"\"\"\n\n    @abstractmethod\n    def __call__(\n        self, splits: int, items: \"List[nodes.Item]\", durations: \"Dict[str, float]\"\n    ) -&gt; \"List[TestGroup]\":\n        pass\n\n    def __hash__(self) -&gt; int:\n        return hash(self.__class__.__name__)\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, AlgorithmBase):\n            return NotImplemented\n        return self.__class__.__name__ == other.__class__.__name__\n</code></pre>"},{"location":"api_docs/#pytest_split.algorithms.DurationBasedChunksAlgorithm","title":"<code>DurationBasedChunksAlgorithm</code>","text":"<p>               Bases: <code>AlgorithmBase</code></p> <p>Split tests into groups by runtime. Ensures tests are split into non-overlapping groups. The original list of test items is split into groups by finding boundary indices i_0, i_1, i_2 and creating group_1 = items[0:i_0], group_2 = items[i_0, i_1], group_3 = items[i_1, i_2], ...</p> <p>:param splits: How many groups we're splitting in. :param items: Test items passed down by Pytest. :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests :return: List of TestGroup</p> Source code in <code>src/pytest_split/algorithms.py</code> <pre><code>class DurationBasedChunksAlgorithm(AlgorithmBase):\n    \"\"\"\n    Split tests into groups by runtime.\n    Ensures tests are split into non-overlapping groups.\n    The original list of test items is split into groups by finding boundary indices i_0, i_1, i_2\n    and creating group_1 = items[0:i_0], group_2 = items[i_0, i_1], group_3 = items[i_1, i_2], ...\n\n    :param splits: How many groups we're splitting in.\n    :param items: Test items passed down by Pytest.\n    :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests\n    :return: List of TestGroup\n    \"\"\"\n\n    def __call__(\n        self, splits: int, items: \"List[nodes.Item]\", durations: \"Dict[str, float]\"\n    ) -&gt; \"List[TestGroup]\":\n        items_with_durations = _get_items_with_durations(items, durations)\n        time_per_group = sum(map(itemgetter(1), items_with_durations)) / splits\n\n        selected: List[List[nodes.Item]] = [[] for i in range(splits)]\n        deselected: List[List[nodes.Item]] = [[] for i in range(splits)]\n        duration: List[float] = [0 for i in range(splits)]\n\n        group_idx = 0\n        for item, item_duration in items_with_durations:\n            if duration[group_idx] &gt;= time_per_group:\n                group_idx += 1\n\n            selected[group_idx].append(item)\n            for i in range(splits):\n                if i != group_idx:\n                    deselected[i].append(item)\n            duration[group_idx] += item_duration\n\n        return [\n            TestGroup(\n                selected=selected[i], deselected=deselected[i], duration=duration[i]\n            )\n            for i in range(splits)\n        ]\n</code></pre>"},{"location":"api_docs/#pytest_split.algorithms.LeastDurationAlgorithm","title":"<code>LeastDurationAlgorithm</code>","text":"<p>               Bases: <code>AlgorithmBase</code></p> <p>Split tests into groups by runtime. It walks the test items, starting with the test with largest duration. It assigns the test with the largest runtime to the group with the smallest duration sum.</p> <p>The algorithm sorts the items by their duration. Since the sorting algorithm is stable, ties will be broken by maintaining the original order of items. It is therefore important that the order of items be identical on all nodes that use this plugin. Due to issue #25 this might not always be the case.</p> <p>:param splits: How many groups we're splitting in. :param items: Test items passed down by Pytest. :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests :return:     List of groups</p> Source code in <code>src/pytest_split/algorithms.py</code> <pre><code>class LeastDurationAlgorithm(AlgorithmBase):\n    \"\"\"\n    Split tests into groups by runtime.\n    It walks the test items, starting with the test with largest duration.\n    It assigns the test with the largest runtime to the group with the smallest duration sum.\n\n    The algorithm sorts the items by their duration. Since the sorting algorithm is stable, ties will be broken by\n    maintaining the original order of items. It is therefore important that the order of items be identical on all nodes\n    that use this plugin. Due to issue #25 this might not always be the case.\n\n    :param splits: How many groups we're splitting in.\n    :param items: Test items passed down by Pytest.\n    :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests\n    :return:\n        List of groups\n    \"\"\"\n\n    def __call__(\n        self, splits: int, items: \"List[nodes.Item]\", durations: \"Dict[str, float]\"\n    ) -&gt; \"List[TestGroup]\":\n        items_with_durations = _get_items_with_durations(items, durations)\n\n        # add index of item in list\n        items_with_durations_indexed = [\n            (*tup, i) for i, tup in enumerate(items_with_durations)\n        ]\n\n        # Sort by name to ensure it's always the same order\n        items_with_durations_indexed = sorted(\n            items_with_durations_indexed, key=lambda tup: str(tup[0])\n        )\n\n        # sort in ascending order\n        sorted_items_with_durations = sorted(\n            items_with_durations_indexed, key=lambda tup: tup[1], reverse=True\n        )\n\n        selected: List[List[Tuple[nodes.Item, int]]] = [[] for _ in range(splits)]\n        deselected: List[List[nodes.Item]] = [[] for _ in range(splits)]\n        duration: List[float] = [0 for _ in range(splits)]\n\n        # create a heap of the form (summed_durations, group_index)\n        heap: List[Tuple[float, int]] = [(0, i) for i in range(splits)]\n        heapq.heapify(heap)\n        for item, item_duration, original_index in sorted_items_with_durations:\n            # get group with smallest sum\n            summed_durations, group_idx = heapq.heappop(heap)\n            new_group_durations = summed_durations + item_duration\n\n            # store assignment\n            selected[group_idx].append((item, original_index))\n            duration[group_idx] = new_group_durations\n            for i in range(splits):\n                if i != group_idx:\n                    deselected[i].append(item)\n\n            # store new duration - in case of ties it sorts by the group_idx\n            heapq.heappush(heap, (new_group_durations, group_idx))\n\n        groups = []\n        for i in range(splits):\n            # sort the items by their original index to maintain relative ordering\n            # we don't care about the order of deselected items\n            s = [\n                item\n                for item, original_index in sorted(selected[i], key=lambda tup: tup[1])\n            ]\n            group = TestGroup(\n                selected=s, deselected=deselected[i], duration=duration[i]\n            )\n            groups.append(group)\n        return groups\n</code></pre>"},{"location":"api_docs/#pytest_split.ipynb_compatibility","title":"<code>ipynb_compatibility</code>","text":""},{"location":"api_docs/#pytest_split.ipynb_compatibility.ensure_ipynb_compatibility","title":"<code>ensure_ipynb_compatibility(group: TestGroup, items: list) -&gt; None</code>","text":"<p>Ensures that group doesn't contain partial IPy notebook cells.</p> <p><code>pytest-split</code> might, in principle, break up the cells of an IPython notebook into different test groups, in which case the tests most likely fail (for starters, libraries are imported in Cell 0, so all subsequent calls to the imported libraries in the following cells will raise <code>NameError</code>).</p> Source code in <code>src/pytest_split/ipynb_compatibility.py</code> <pre><code>def ensure_ipynb_compatibility(group: \"TestGroup\", items: list) -&gt; None:  # type: ignore[type-arg]\n    \"\"\"\n    Ensures that group doesn't contain partial IPy notebook cells.\n\n    ``pytest-split`` might, in principle, break up the cells of an\n    IPython notebook into different test groups, in which case the tests\n    most likely fail (for starters, libraries are imported in Cell 0, so\n    all subsequent calls to the imported libraries in the following cells\n    will raise ``NameError``).\n\n    \"\"\"\n    if not group.selected or not _is_ipy_notebook(group.selected[0].nodeid):\n        return\n\n    item_node_ids = [item.nodeid for item in items]\n\n    # Deal with broken up notebooks at the beginning of the test group\n    first = group.selected[0].nodeid\n    siblings = _find_sibiling_ipynb_cells(first, item_node_ids)\n    if first != siblings[0]:\n        for item in list(group.selected):\n            if item.nodeid in siblings:\n                group.deselected.append(item)\n                group.selected.remove(item)\n\n    if not group.selected or not _is_ipy_notebook(group.selected[-1].nodeid):\n        return\n\n    # Deal with broken up notebooks at the end of the test group\n    last = group.selected[-1].nodeid\n    siblings = _find_sibiling_ipynb_cells(last, item_node_ids)\n    if last != siblings[-1]:\n        for item in list(group.deselected):\n            if item.nodeid in siblings:\n                group.deselected.remove(item)\n                group.selected.append(item)\n</code></pre>"},{"location":"api_docs/#pytest_split.plugin","title":"<code>plugin</code>","text":""},{"location":"api_docs/#pytest_split.plugin.Base","title":"<code>Base</code>","text":"Source code in <code>src/pytest_split/plugin.py</code> <pre><code>class Base:\n    def __init__(self, config: \"Config\") -&gt; None:\n        \"\"\"\n        Load durations and set up a terminal writer.\n\n        This logic is shared for both the split- and cache plugin.\n        \"\"\"\n        self.config = config\n        self.writer = create_terminal_writer(self.config)\n\n        try:\n            with open(config.option.durations_path) as f:\n                self.cached_durations = json.loads(f.read())\n        except FileNotFoundError:\n            self.cached_durations = {}\n\n        # This code provides backwards compatibility after we switched\n        # from saving durations in a list-of-lists to a dict format\n        # Remove this when bumping to v1\n        if isinstance(self.cached_durations, list):\n            self.cached_durations = dict(self.cached_durations)\n</code></pre>"},{"location":"api_docs/#pytest_split.plugin.Base.__init__","title":"<code>__init__(config: Config) -&gt; None</code>","text":"<p>Load durations and set up a terminal writer.</p> <p>This logic is shared for both the split- and cache plugin.</p> Source code in <code>src/pytest_split/plugin.py</code> <pre><code>def __init__(self, config: \"Config\") -&gt; None:\n    \"\"\"\n    Load durations and set up a terminal writer.\n\n    This logic is shared for both the split- and cache plugin.\n    \"\"\"\n    self.config = config\n    self.writer = create_terminal_writer(self.config)\n\n    try:\n        with open(config.option.durations_path) as f:\n            self.cached_durations = json.loads(f.read())\n    except FileNotFoundError:\n        self.cached_durations = {}\n\n    # This code provides backwards compatibility after we switched\n    # from saving durations in a list-of-lists to a dict format\n    # Remove this when bumping to v1\n    if isinstance(self.cached_durations, list):\n        self.cached_durations = dict(self.cached_durations)\n</code></pre>"},{"location":"api_docs/#pytest_split.plugin.PytestSplitCachePlugin","title":"<code>PytestSplitCachePlugin</code>","text":"<p>               Bases: <code>Base</code></p> <p>The cache plugin writes durations to our durations file.</p> Source code in <code>src/pytest_split/plugin.py</code> <pre><code>class PytestSplitCachePlugin(Base):\n    \"\"\"\n    The cache plugin writes durations to our durations file.\n    \"\"\"\n\n    def pytest_sessionfinish(self) -&gt; None:\n        \"\"\"\n        Method is called by Pytest after the test-suite has run.\n        https://github.com/pytest-dev/pytest/blob/main/src/_pytest/main.py#L308\n        \"\"\"\n        terminal_reporter = self.config.pluginmanager.get_plugin(\"terminalreporter\")\n        test_durations: Dict[str, float] = {}\n\n        for test_reports in terminal_reporter.stats.values():  # type: ignore[union-attr]\n            for test_report in test_reports:\n                if isinstance(test_report, TestReport):\n                    # These ifs be removed after this is solved: # https://github.com/spulec/freezegun/issues/286\n                    if test_report.duration &lt; 0:\n                        continue  # pragma: no cover\n                    if (\n                        test_report.when in (\"teardown\", \"setup\")\n                        and test_report.duration\n                        &gt; STORE_DURATIONS_SETUP_AND_TEARDOWN_THRESHOLD\n                    ):\n                        # Ignore not legit teardown durations\n                        continue  # pragma: no cover\n\n                    # Add test durations to map\n                    if test_report.nodeid not in test_durations:\n                        test_durations[test_report.nodeid] = 0\n                    test_durations[test_report.nodeid] += test_report.duration\n\n        if self.config.option.clean_durations:\n            self.cached_durations = dict(test_durations)\n        else:\n            for k, v in test_durations.items():\n                self.cached_durations[k] = v\n\n        with open(self.config.option.durations_path, \"w\") as f:\n            json.dump(self.cached_durations, f, sort_keys=True, indent=4)\n\n        message = self.writer.markup(\n            f\"\\n\\n[pytest-split] Stored test durations in {self.config.option.durations_path}\"\n        )\n        self.writer.line(message)\n</code></pre>"},{"location":"api_docs/#pytest_split.plugin.PytestSplitCachePlugin.pytest_sessionfinish","title":"<code>pytest_sessionfinish() -&gt; None</code>","text":"<p>Method is called by Pytest after the test-suite has run. https://github.com/pytest-dev/pytest/blob/main/src/_pytest/main.py#L308</p> Source code in <code>src/pytest_split/plugin.py</code> <pre><code>def pytest_sessionfinish(self) -&gt; None:\n    \"\"\"\n    Method is called by Pytest after the test-suite has run.\n    https://github.com/pytest-dev/pytest/blob/main/src/_pytest/main.py#L308\n    \"\"\"\n    terminal_reporter = self.config.pluginmanager.get_plugin(\"terminalreporter\")\n    test_durations: Dict[str, float] = {}\n\n    for test_reports in terminal_reporter.stats.values():  # type: ignore[union-attr]\n        for test_report in test_reports:\n            if isinstance(test_report, TestReport):\n                # These ifs be removed after this is solved: # https://github.com/spulec/freezegun/issues/286\n                if test_report.duration &lt; 0:\n                    continue  # pragma: no cover\n                if (\n                    test_report.when in (\"teardown\", \"setup\")\n                    and test_report.duration\n                    &gt; STORE_DURATIONS_SETUP_AND_TEARDOWN_THRESHOLD\n                ):\n                    # Ignore not legit teardown durations\n                    continue  # pragma: no cover\n\n                # Add test durations to map\n                if test_report.nodeid not in test_durations:\n                    test_durations[test_report.nodeid] = 0\n                test_durations[test_report.nodeid] += test_report.duration\n\n    if self.config.option.clean_durations:\n        self.cached_durations = dict(test_durations)\n    else:\n        for k, v in test_durations.items():\n            self.cached_durations[k] = v\n\n    with open(self.config.option.durations_path, \"w\") as f:\n        json.dump(self.cached_durations, f, sort_keys=True, indent=4)\n\n    message = self.writer.markup(\n        f\"\\n\\n[pytest-split] Stored test durations in {self.config.option.durations_path}\"\n    )\n    self.writer.line(message)\n</code></pre>"},{"location":"api_docs/#pytest_split.plugin.PytestSplitPlugin","title":"<code>PytestSplitPlugin</code>","text":"<p>               Bases: <code>Base</code></p> Source code in <code>src/pytest_split/plugin.py</code> <pre><code>class PytestSplitPlugin(Base):\n    def __init__(self, config: \"Config\"):\n        super().__init__(config)\n\n        if not self.cached_durations:\n            message = self.writer.markup(\n                \"\\n[pytest-split] No test durations found. Pytest-split will \"\n                \"split tests evenly when no durations are found. \"\n                \"\\n[pytest-split] You can expect better results in consequent runs, \"\n                \"when test timings have been documented.\\n\"\n            )\n            self.writer.line(message)\n\n    @hookimpl(trylast=True)\n    def pytest_collection_modifyitems(\n        self, config: \"Config\", items: \"List[nodes.Item]\"\n    ) -&gt; None:\n        \"\"\"\n        Collect and select the tests we want to run, and deselect the rest.\n        \"\"\"\n        splits: int = config.option.splits\n        group_idx: int = config.option.group\n\n        algo = algorithms.Algorithms[config.option.splitting_algorithm].value\n        groups = algo(splits, items, self.cached_durations)\n        group = groups[group_idx - 1]\n\n        ensure_ipynb_compatibility(group, items)\n\n        items[:] = group.selected\n        config.hook.pytest_deselected(items=group.deselected)\n\n        self.writer.line(\n            self.writer.markup(\n                f\"\\n\\n[pytest-split] Splitting tests with algorithm: {config.option.splitting_algorithm}\"\n            )\n        )\n        self.writer.line(\n            self.writer.markup(\n                f\"[pytest-split] Running group {group_idx}/{splits} (estimated duration: {group.duration:.2f}s)\\n\"\n            )\n        )\n</code></pre>"},{"location":"api_docs/#pytest_split.plugin.PytestSplitPlugin.pytest_collection_modifyitems","title":"<code>pytest_collection_modifyitems(config: Config, items: List[nodes.Item]) -&gt; None</code>","text":"<p>Collect and select the tests we want to run, and deselect the rest.</p> Source code in <code>src/pytest_split/plugin.py</code> <pre><code>@hookimpl(trylast=True)\ndef pytest_collection_modifyitems(\n    self, config: \"Config\", items: \"List[nodes.Item]\"\n) -&gt; None:\n    \"\"\"\n    Collect and select the tests we want to run, and deselect the rest.\n    \"\"\"\n    splits: int = config.option.splits\n    group_idx: int = config.option.group\n\n    algo = algorithms.Algorithms[config.option.splitting_algorithm].value\n    groups = algo(splits, items, self.cached_durations)\n    group = groups[group_idx - 1]\n\n    ensure_ipynb_compatibility(group, items)\n\n    items[:] = group.selected\n    config.hook.pytest_deselected(items=group.deselected)\n\n    self.writer.line(\n        self.writer.markup(\n            f\"\\n\\n[pytest-split] Splitting tests with algorithm: {config.option.splitting_algorithm}\"\n        )\n    )\n    self.writer.line(\n        self.writer.markup(\n            f\"[pytest-split] Running group {group_idx}/{splits} (estimated duration: {group.duration:.2f}s)\\n\"\n        )\n    )\n</code></pre>"},{"location":"api_docs/#pytest_split.plugin.pytest_addoption","title":"<code>pytest_addoption(parser: Parser) -&gt; None</code>","text":"<p>Declare pytest-split's options.</p> Source code in <code>src/pytest_split/plugin.py</code> <pre><code>def pytest_addoption(parser: \"Parser\") -&gt; None:\n    \"\"\"\n    Declare pytest-split's options.\n    \"\"\"\n    group = parser.getgroup(\n        \"Split tests into groups which execution time is about the same. \"\n        \"Run with --store-durations to store information about test execution times.\"\n    )\n    group.addoption(\n        \"--store-durations\",\n        dest=\"store_durations\",\n        action=\"store_true\",\n        help=\"Store durations into '--durations-path'.\",\n    )\n    group.addoption(\n        \"--durations-path\",\n        dest=\"durations_path\",\n        help=(\n            \"Path to the file in which durations are (to be) stored, \"\n            \"default is .test_durations in the current working directory\"\n        ),\n        default=os.path.join(os.getcwd(), \".test_durations\"),\n    )\n    group.addoption(\n        \"--splits\",\n        dest=\"splits\",\n        type=int,\n        help=\"The number of groups to split the tests into\",\n    )\n    group.addoption(\n        \"--group\",\n        dest=\"group\",\n        type=int,\n        help=\"The group of tests that should be executed (first one is 1)\",\n    )\n    group.addoption(\n        \"--splitting-algorithm\",\n        dest=\"splitting_algorithm\",\n        type=str,\n        help=f\"Algorithm used to split the tests. Choices: {algorithms.Algorithms.names()}\",\n        default=\"duration_based_chunks\",\n        choices=algorithms.Algorithms.names(),\n    )\n    group.addoption(\n        \"--clean-durations\",\n        dest=\"clean_durations\",\n        action=\"store_true\",\n        help=(\n            \"Removes the test duration info for tests which are not present \"\n            \"while running the suite with '--store-durations'.\"\n        ),\n    )\n</code></pre>"},{"location":"api_docs/#pytest_split.plugin.pytest_cmdline_main","title":"<code>pytest_cmdline_main(config: Config) -&gt; Optional[Union[int, ExitCode]]</code>","text":"<p>Validate options.</p> Source code in <code>src/pytest_split/plugin.py</code> <pre><code>@pytest.hookimpl(tryfirst=True)\ndef pytest_cmdline_main(config: \"Config\") -&gt; \"Optional[Union[int, ExitCode]]\":\n    \"\"\"\n    Validate options.\n    \"\"\"\n    group = config.getoption(\"group\")\n    splits = config.getoption(\"splits\")\n\n    if splits is None and group is None:\n        return None\n\n    if splits and group is None:\n        raise pytest.UsageError(\"argument `--group` is required\")\n\n    if group and splits is None:\n        raise pytest.UsageError(\"argument `--splits` is required\")\n\n    if splits &lt; 1:\n        raise pytest.UsageError(\"argument `--splits` must be &gt;= 1\")\n\n    if group &lt; 1 or group &gt; splits:\n        raise pytest.UsageError(f\"argument `--group` must be &gt;= 1 and &lt;= {splits}\")\n\n    return None\n</code></pre>"},{"location":"api_docs/#pytest_split.plugin.pytest_configure","title":"<code>pytest_configure(config: Config) -&gt; None</code>","text":"<p>Enable the plugins we need.</p> Source code in <code>src/pytest_split/plugin.py</code> <pre><code>def pytest_configure(config: \"Config\") -&gt; None:\n    \"\"\"\n    Enable the plugins we need.\n    \"\"\"\n    if config.option.splits and config.option.group:\n        config.pluginmanager.register(PytestSplitPlugin(config), \"pytestsplitplugin\")\n\n    if config.option.store_durations:\n        config.pluginmanager.register(\n            PytestSplitCachePlugin(config), \"pytestsplitcacheplugin\"\n        )\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#0100-2024-10-16","title":"0.10.0 - 2024-10-16","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Support for Python 3.13.</li> </ul>"},{"location":"changelog/#090-2024-06-19","title":"0.9.0 - 2024-06-19","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Cruft update to get up to date with the parent cookiecutter template</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Support for Python 3.7</li> </ul>"},{"location":"changelog/#082-2024-01-29","title":"0.8.2 - 2024-01-29","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Support for pytest 8.x</li> <li>Python 3.12 to CI test matrix</li> </ul>"},{"location":"changelog/#081-2023-04-12","title":"0.8.1 - 2023-04-12","text":""},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Introduce Ruff</li> <li>Fixed usage of deprecated pytest API</li> </ul>"},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Python 3.11 to CI test matrix</li> </ul>"},{"location":"changelog/#080-2022-04-22","title":"0.8.0 - 2022-04-22","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>The <code>least_duration</code> algorithm should now split deterministically regardless of starting test order.   This should fix the main problem when running with test-randomization packages such as <code>pytest-randomly</code> or <code>pytest-random-order</code>   See #52</li> </ul>"},{"location":"changelog/#070-2022-03-13","title":"0.7.0 - 2022-03-13","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Support for pytest 7.x, see https://github.com/jerry-git/pytest-split/pull/47</li> </ul>"},{"location":"changelog/#060-2022-01-10","title":"0.6.0 - 2022-01-10","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>PR template</li> <li>Test against 3.10</li> <li>Compatibility with IPython Notebooks</li> </ul>"},{"location":"changelog/#050-2021-11-09","title":"0.5.0 - 2021-11-09","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Wolt cookiecutter + cruft setup, see https://github.com/jerry-git/pytest-split/pull/33</li> </ul>"},{"location":"changelog/#040-2021-11-09","title":"0.4.0 - 2021-11-09","text":""},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Durations file content in prettier format, see https://github.com/jerry-git/pytest-split/pull/31</li> </ul>"}]}